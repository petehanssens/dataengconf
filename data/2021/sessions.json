[
  {
    "groupId": null,
    "groupName": "All",
    "sessions": [
      {
        "questionAnswers": [],
        "id": "272217",
        "title": "A Single Data Platform for All of Your Workloads",
        "description": "Organisations struggle to balance the competing requirements of the business, data scientists, data engineers, data analysts, risk and security experts, and the finance department.  Maintaining multiple data platforms to try to keep everyone happy often leaves many people unhappy.  Learn how customers are using Snowflake to simplify their data at rest, data in motion, and data science workloads on a single and secure data platform.   ",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "0d41131c-4737-4725-a37d-7ca1493b08ec",
            "name": "Stephen Weingartner"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "271932",
        "title": "Albero - Decision Tree “when to use what – the full Azure data estate",
        "description": "This year a lot of attention is focused on data and data services. Microsoft is doing their absolute best to help our customers benefit from the wide portfolio of Azure Data Services. But what if this amount of services is a bit too many for many of us not to mention our customers? We have more than 20 main data services plus SKUs. Navigation across such a wide range of technologies and answering simple question: “When to choose what data technology?” suddenly became quite a complex task even for experts.  Andrei, Elizabeth and Eleni joined hands in creating universal and easy to consume bird-eye view of the entire Azure data services. This decision tree helps customers/partners to define and shape their thought process around selection of the certain data technologies and using them together in building data solutions.",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "a7126dd4-e7b6-4bca-a348-e722c63d6f23",
            "name": "Elizabeth Antoine"
          },
          {
            "id": "06bb0e75-cae8-4216-83c8-0a2e7288b6da",
            "name": "Andrei Zaichikov"
          },
          {
            "id": "3c8bc5ec-15a8-4026-b4d7-f5857e3cf1fa",
            "name": "Eleni Santorinaiou"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "273972",
        "title": "An open data platform without the learning curve",
        "description": "Introducing a new open source data platform that allows you to be immediately productive without either shipping your data outside your AWS account, or climbing any steep learning curve.\r\n\r\nBecome productive in minutes from scratch - built on top of AWS Athena, AWS CDK & AWS Glue.",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "d82d31c9-3c59-4ee5-af07-5778caf0e5dc",
            "name": "Timothy Downs"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "274291",
        "title": "Beginners guide to Azure MLOps",
        "description": "A commercial scenario and example of work we have recently done for a Large engineering client and lessons learned around implementing Azure MLOps. \r\n\r\nWith a Technical how-to of:\r\n\r\n•\tCreate reproducible ML pipelines.\r\n•\tCreate reusable software environments \r\n•\tRegister, package, and deploy models from anywhere. \r\n•\tCapture the governance data for the end-to-end ML lifecycle. \r\n•\tNotify and alert on events in the ML lifecycle. \r\n•\tMonitor ML applications for operational and ML-related\r\nissues. \r\n•\tAutomate the end-to-end ML lifecycle with Azure Machine Learning and Azure Pipelines. \r\n",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "300098b5-576f-4028-8014-51a562193190",
            "name": "Chris Benson"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "271311",
        "title": "Better, Faster, Stronger Streaming: Your First Dive into Flink SQL",
        "description": "For the most flexible, powerful stream processing engines, it seems like the barrier to entry has never been higher than it is now. If you’ve tried, or have been interested in leveraging the strengths of real-time data processing - maybe for machine learning, IoT, anomaly detection or data analysis - but you’ve been held back: I’ve been there, and it’s frustrating. And that’s why this talk is for you. \r\n\r\nThat being said, this talk is also for you if you ARE experienced with stream processing but you want an easy (and if I say so myself, pretty fun) way to add some of the newest, bleeding edge features to your toolbelt.\r\n\r\nThis session will be about getting started with Flink SQL. Apache Flink’s high level SQL language has the familiarity of the SQL you know and love (or at least, know…), but with some powerful new functionality, and of course, the benefit of being able to be used with Flink and PyFlink. \r\n\r\nMore specifically, this will be a pragmatic entry into creating data pipelines with Flink SQL, as well as a sneak peek into some of its newest and most interesting features.\r\n\r\n",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "509cd132-a8aa-40f1-8c81-31d3421bd4e3",
            "name": "Caito Scherr"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "261511",
        "title": "Building a Data Platform at Assembly Payments",
        "description": "- Datalake journey @ Assembly Payments.\r\n- Description of Architecture using RDS, DynamoDB, DMS, Firehose, Lambda, S3, Glue, EventBridge, Step Functions, Fargate, DBT, Snowflake and Looker\r\n- Share the benefits we felt like event driven, serverless, performance, low cost and lower operational overhead.\r\n- and share some of the learnings in the usage of step functions, glue, incremental loads and dependency management.",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "88e9b4d4-cd98-48aa-9eb5-93ff5a8307c6",
            "name": "Roopak Madhu"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "265756",
        "title": "Chain-speed inference for Computer Vision Pipelines",
        "description": "I will go through some best practices for minimizing real-time prediction latency for devices in processing plants. \r\n\r\nI will cover data collection, processing, data pipelines, model deployment, and monitoring. I would also like to discuss common challenges that arise in factories: a poor network connection, space limitations, and keeping up with the chain-speed.\r\n\r\n",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "03147c4d-27f7-4575-b577-eb80be446935",
            "name": "Mercedes Speroni"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "272064",
        "title": "Data Engineers: Privacy Is Your Problem",
        "description": "Data has the ability to transform and automate business processes. But data also comes with a responsibility to keep people’s personal information protected and ensure it is used in an ethical manner. It is the responsibility of data engineers to instinctively identify and separate dangerous data from the benign.\r\n\r\nIn this session, Stephen Bailey PhD, director of data and analytics for Immuta, will discuss the need for data engineers to take on the responsibility of data privacy. While all organizations working with data understand the seriousness of data privacy, many aren’t sure who is responsible for protecting it. Stephen will explain why managing privacy loss is something only data engineers can solve, as data engineers are the ones who created the systems. \r\n\r\nHe will outline a suggested new set of engineering best practices that go beyond the domains of security and system design. These best practices will include understanding the strengths and weaknesses of data masking, learning anonymization techniques like k-anonymization and differential privacy,. Ultimately, data engineers should know the practice of privacy by design as intuitively as they do the principle of least privilege. \r\n\r\nAttendees of this educational session will walk away with a better understanding of how they can improve data privacy without compromising data access control. Threats to data aren’t slowing down, so it’s time to fight against them. \r\n",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "b7ff99fc-1f9f-4c6f-9e9e-1cb46898791c",
            "name": "Stephen  Bailey"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "262076",
        "title": "Data Quality with Great Expectations and Airflow in a Reverse-ETL World",
        "description": "Data-driven companies are asking their analytics teams to expose information in the data warehouse to third-party applications used by others in the organization. With analytics workflows having increasing downstream dependencies, data quality testing becomes of utmost importance. In this talk, we’ll walk through leveraging the Great Expectations library within a data processing workflow in Airflow. With this architecture, we will establish a gateway to ensure bad data is not exposed downstream while also notifying the team when data quality tests fail.",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "fc85e3db-bf1d-48f2-89b2-d06c49485f12",
            "name": "Sarah Krasnik"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "272212",
        "title": "Data quality: the key to long term happiness",
        "description": "The advent of modern data warehousing has equalled the playing field and pivoted a focus from data volume to data quality instead. This talk aims to practically explore approaches to quantify data quality at various stages in the collection and processing lifecycle and to present tools that can be implemented to help in the fight against erroneous data.",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "5fce6ed5-e1cc-46e0-adec-dc5aa9a039f5",
            "name": "Mike Robins"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "270127",
        "title": "From experiment to production - a journey of a machine learning model",
        "description": "Machine learning has a wider and wider adoption in the industry as a technology to build intelligent and data enriched systems. However, the reality is that machine learning practice often stops at the end of rapid experiments before its value can be harvested in the real world. This is because the process and the culture of taking ML models from experiment to production is lacking.\r\n\r\nA machine learning project typically consists of experiment, development and deployment three main phrases. Correspondingly, a machine learning system consists of processes and components that facilitate the operations and data flows in those three main phrases. This system provides a framework and best practices for machine learning practitioners to develop models from experiment to production.\r\n\r\nThis talk explores the process pattern of getting a machine learning model into production and the system to support this operation. An example of productionising a natural language processing model will be used to illustrate how a model travels through experiment tracking, data artifacts management, data / machine learning pipelines and goes into production. The talk will also provide examples of tooling and patterns to be used at each stage.\r\n",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "34d4e7a6-895c-4c67-a69f-e01bda9dc109",
            "name": "Xin Liang"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "271937",
        "title": "From telescope to data centre: adventures in astronomical data pipelines",
        "description": "I will describe the data flow we have developed to move astronomical data from Siding Spring Observatory in central west NSW to data centres in Sydney and Canberra, and the subsequent processing stages to produce science-ready data. This is a story that involves the coupling of Apache Nifi, MongoDB and Docker with astronomical data reduction software to allow researchers to explore some of the most violent phenomena in the Universe: colliding black holes and exploding stars. I’ll also look at some of the related future data engineering challenges involved in the Square Kilometre Array and the European Southern Observatory. ",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "06417bf4-83bb-4535-b2ff-cfda644efb96",
            "name": "Simon O'Toole"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "272068",
        "title": "Gone Streaming: dbt+Materialize in 10 minutes",
        "description": "dbt is great for batch, but it can only approximate transforming streaming data. Together with the dbt community, we’ve worked on an adapter that allows you to transform your streaming data in real-time using Materialize as your data warehouse. What does this mean in practice? The first time you run a dbt model on top of Materialize…well, you never have to run it again! No matter how much or how frequently your data arrives, your model will stay up to date. No matter when you query your view, it will always return a fresh answer. Excited? Skeptical? Cautiously optimistic? Join us to see it for yourself as we walk you through a demo!",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "41a79c57-29ee-4f51-9410-548f320f0160",
            "name": "Jessica Laughlin"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "252604",
        "title": "Inside Apache Druid's Storage and Query Engine",
        "description": "Apache Druid is an open-source columnar database known for high performance at scale; its largest deployments comprise thousands of servers. But no matter the scale, high performance starts with good fundamentals. This talk will dive into those fundamentals by exploring the inner workings of a single data server. We’ll cover how Apache Druid stores data, what kinds of compression it uses, how it indexes data, how the storage engine is linked with the query processing engine, and how the system handles resource management and multithreading. Together, all these pieces enable Apache Druid to process billions of records per second on a single data server.\r\n",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "40cb614a-1c9b-4c44-b4b0-80e9521509d7",
            "name": "Gian Merlino"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "270738",
        "title": "Intelligent Serverless and Scalable Real-Time Data Pipeline using Kinesis, Fargate and CFN",
        "description": "This session is about a real case study of an intelligent serverless real-time data pipeline. This is implemented for a big digital media client. The session will cover business problems, approach to cater the problem, architecture, implemented solution, and value created out from the implemented solution. The solution is based on the AWS serverless approach in a highly scalable manner following all well architect principles.\r\npresentation- https://www.slideshare.net/YogeshSharma208/intelligent-serverlessstreamingpipelineusingkinesisfargatecfn\r\n",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "8cfa8e05-dcf8-464b-9d3b-1743cacdc0ed",
            "name": "Yogesh Sharma"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "261783",
        "title": "Kickstarting a greenfield data project",
        "description": "My takeaways building a data platform from the ground up",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "0d2ed788-6375-4e75-b724-0f28e0ea568d",
            "name": "Vinay Kulkarni"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "271198",
        "title": "Logging Apache Spark - How we made it easy",
        "description": "Looking at our metrics on Graphite is pretty nice, but what about our logs? How do you improve the visibility of your logs while running Spark on EMR? If you're tired of ssh-ing into your servers and searching log files, this architecture design is exactly for you. ",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "7aae10e0-5083-42fa-a1d2-bb8d3b56c55c",
            "name": "Simona Meriam"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "272193",
        "title": "Metrics-driven Data Architectures",
        "description": "The gravitational pull of cloud data warehouses is a powerful force on data platform architecture. This is evidenced by the growing use of data warehouses to build and serve data products and the well-established shift from ETL to ELT transformation patterns. More recently the trend of pushing data transformations to inside the data warehouse is playing out on the consumption side, with business metric definition and calculation being shifted from fragmented locations across multiple BI tools and data science notebooks into the data warehouse, where they can be published for use across the organisation by a range of consumers.   \r\n\r\nThis trend has recently coalesced around the notion of a metrics layer within the modern data stack. In this talk I will unpack the challenges motivating this change in data architecture and identify the core features of the metrics layer that meets these needs. I will look at different flavours of data architecture that enable these features by surveying existing OSS tools and vendor offerings. In doing so, I will address the questions of how this notion of a metrics layer differs from existing approaches to OLAP databases, and whether data warehouses are the appropriate place to be building metric layers. ",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "8f2f01ec-4972-49fe-a204-8f226fa209cb",
            "name": "Ned Letcher"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "260553",
        "title": "Modern Data Warehouse for Small and Medium Business",
        "description": "With many organizations in SMB sector looking at opportunity to benefit from using modern big data technologies and tools within their budget and skill set, this session overviews an opinion on appropriate architecture for this use-case. ",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "23f8221e-bfb9-40ec-af09-a724b5ec9aa3",
            "name": "Galina Polyakova"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "260573",
        "title": "Object Compaction in Cloud for High Yield",
        "description": "In file systems, large sequential writes are more beneficial than small random writes, and hence many storage systems implement a log structured file system. In the same way, the cloud favors large objects more than small objects. Cloud providers place throttling limits on PUTs and GETs, and so it takes significantly longer time to upload a bunch of small objects than a large object of the aggregate size. Moreover, there are per-PUT calls associated with uploading smaller objects.\r\nIn Netflix, a lot of media assets and their relevant metadata is generated and pushed to cloud. Most of these files are between 10s of bytes to 10s of kilobytes and are saved as small objects on Cloud.\r\nIn this talk, we would like to propose a strategy to compact these small objects into larger blobs before uploading them to Cloud. We will discuss the policies to select relevant smaller objects, and how to manage the indexing of these objects within the blob. We will also discuss how different cloud storage operations such as reads and deletes would be implemented for such objects. This includes recycling blobs that have dead small objects - due to overwrites, etc.\r\nFinally, we would showcase the potential impact of such a strategy on Netflix assets in terms of cost and performance.\r\n",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "0e577c69-f95e-4044-a53c-d626732f2bb6",
            "name": "Tejas Chopra"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "267957",
        "title": "Reliable data engineering made easy",
        "description": "Organisations have a wealth of information siloed in various data sources. These could vary from databases (Oracle, MySQL, Postgres, etc) to product applications (Salesforce, Marketo, HubSpot, etc). A significant number of use-cases need data from these diverse data sources to produce meaningful reports and predictions. \r\nFor many years, organisations tried to centrally collect all their data in the data warehouse but these were not suited or were too expensive for handling unstructured data, semi-structured data, and data with high variety, velocity, and volume. It also limited the types of analytics data teams could use; unable to do machine learning or anything more than basic SQL.\r\n\r\nDelta Lake, released and open-sourced in 2019, is helping thousands of organisations build central data repositories in an open format much more reliably and efficiently than before. Delta Lake provides ACID transactions and efficient indexing that is critical for exposing the data for various access patterns, ranging from ad-hoc SQL queries in BI tools, to scheduled ML jobs. \r\nThis session will provide the one-two step of Data Ingestion & Quality made easy with DELTA Live Tables.\r\n",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "6a4d6c7e-4d03-4cc6-8beb-3b21225f681e",
            "name": "Mahdi Askari"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "259218",
        "title": "Scaling Proximity Targeting via Delta Lakehouse based Data Platforms Ecosystem",
        "description": "Proximity Targeting is a marketing technique that uses mobile location services to reach consumers in real-time when they are around a store location or point of interest. This is done by defining a radius around a specific location. If a consumer has opted into location services on their mobile phone and enters within this radius, proximity targeting helps in triggering an advertisement or message to consumers in an effort to influence their behaviour. This can be combined with the ability to purchase impressions through programmatic ad platforms that are powered by real-time bidding which can help businesses formulate the right strategy of influencing their users on a particular geographical area. They can build user groups based on certain characteristics (such as neighbourhoods, demographics, interests, and other data), and subsequently launch another campaign that targets anyone with those characteristics.\r\nThe growth of mobile devices has led to enormous data generation which offers tremendous potential when used effectively for business. Thus we need an efficient platform where we can process such huge data efficiently and with minimum latency and cost. This talk describes MIQ's journey into building a fast, scalable & cost effective processing platform using Spar, MLLib, Kafka's Event Driven microservices, Delta Lakehouse architecture, delivering faster and actionable insights for Proximity targeting which has empowered the creation of a product generating ~30 million dollar revenue on a year to year basis.",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "d8609cd7-b406-4dda-a464-07ddd00183f9",
            "name": "Rohit Srivastava"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "269058",
        "title": "Schema Management: the elephant in the Cloud room",
        "description": "Schema management is a key component in every big Event Streaming platform. The Schema Registry solution has several advantages: better Data Quality, more performant, Data Evolvability, etc. But when we are working in a multi-datacenter or multi-cloud environments, things start to get complicated.\r\n\r\nHow do we replicate schemas between different Kafka clusters in separate regions? How do we ensure compatibility even when consumers are using separate Schema Registries? What can we do when we don't have a Schema Registry in one of the sites?\r\n\r\nIn the last year, Antón has been heavily involved building a platform covering two different Clouds and eight on-prem Data Centers. With that experience, he will review the state-of-the-art of the solutions to this problem and what they did.\r\n\r\nIf you are working with the Schema Registry and you would like to consume or produce your events in the Cloud, this talk will help you with some fresh ideas and the lessons learnt in a similar project.",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "cdc549e0-9c5b-4eb2-b561-dbbecedb76af",
            "name": "Antón Rodríguez"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "270619",
        "title": "Shift-left testing : Building reliable Data Pipelines",
        "description": "Unreliable data pipelines can result in data downtime. “Data downtime“refers to periods of time when your data  is partial, erroneous, missing or otherwise inaccurate. Data driven organisations may need to pay a heavy cost of low trust data. One of the challenges of building reliable  data pipelines is unexpected data coming from different sources.\r\nIn this talk we will share our  experiences of  how we  can build reliable data pipelines  by \r\n\t 1.  “Shifting Left” - testing   to the early stages of  development life cycle  as well as  \r\n         2.  “Shifting-Left” - validations using data contracts\r\nWe will then conclude by discussing  how we can we make sure that the system catches unexpected data and is able to recover from it. ",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "87766859-1911-4965-b57d-26682d22ff3a",
            "name": "Harmeet Sokhi"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "277489",
        "title": "Snowflake and dbt -- Our Journey to the Cloud",
        "description": "nib's journey of using Snowflake and dbt ",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "980689f0-3583-4527-8f05-8b97bcf77512",
            "name": "Pip Sidaway"
          },
          {
            "id": "9e4787a5-3943-41c7-bcc9-13f96e908993",
            "name": "Juliana Zhu"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "271047",
        "title": "Snowpipe Integration via Yaml",
        "description": "Snowpipe Integration via Yaml\r\n\r\n\r\nElevator Pitch(Short Description)...\r\nIf a project uses AWS and Snowflake, Snowpipe is by far the best way for continuous integration. While working  with one of our clients (who happens to be a fan of AWS and Snowflake) we came up with a revolutionary way to set up snowpipe integration for any project using just 10 to 15 lines of Yaml file. Yes, you heard me correctly a yaml to deploy snowpipe not SQL.\r\n\r\n\r\nAbstract(Long Description)...\r\nLeverage Snowpipe’s continuous integration property.. \r\nIn this presentation I’ll share techniques to overcome:-\r\nYou are running SQL every time you run a pipeline, hence unnecessary SQL execution.\r\nHard to maintain SQL queries if a project doesn't have any SQL platform.\r\nThis was very specific for this project, with every new changes/branch we end up in creating a number of artifacts and when we merge the changes to master we literally destroy everything and recreate the same stuff again which ends up causing downtime for customer heavy projects.\r\nEveryone likes less bits of code whether it's SQL, code etc.\r\nWhat problems does it solved:-\r\nBeautiful is better than ugly. YAML really looks more niche than SQL.\r\nFlags used saved us lot of time and execution if already pipe exits when it Create or if Update only run necessary SQL and Delete for housekeeping \r\nThe happiest of them all where the people whole were devops, they love yaml more than anything.\r\nNo outage if no changes made to the snowpipe artifacts.\r\n",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "e2cbe151-a432-48ba-9526-4539d3ed36dd",
            "name": "Suprabhat Sinha"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "264570",
        "title": "Streaming data analytics with Apache Flink",
        "description": "Real-time analytics are on the rise. Apache Flink is a popular purpose-built framework and distributed processing engine for large scale low latency data processing in real-time. In this session, we will give you a brief overview of this popular framework, and a demo to build your first stream processing application with Apache Flink on Amazon Kinesis Data Analytics Studio. ",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "38aa4954-1171-43f1-b9ec-dc6778fea6b8",
            "name": "Masudur Rahaman Sayem"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "272235",
        "title": "Streams, SQL, Action! Up & Running with Materialize",
        "description": "Streaming has changed the way we build and think about data pipelines, but at what cost? In this talk, we’ll introduce Materialize, a streaming database that lets you use standard SQL to query streams of data and get low-latency, incrementally updated answers as the underlying data changes. We’ll cover the basic concepts behind Materialize, how it fits in the “modern data stack” (think dbt, Fivetran, Metabase!) and what makes it unique in comparison to other tools. To tie it all together, we’ll build a simple streaming analytics pipeline — from data ingestion to visualization!",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "d4783611-1bc2-4254-b589-ccb75e28a553",
            "name": "Marta Paes"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "271049",
        "title": "Teleport Data: The future of data",
        "description": "This session will discuss:\r\n- the new data stack\r\n- challenges & problems still faced\r\n- teleport data",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "69038dea-9d70-458d-8604-3b74c71e43bc",
            "name": "John Cosgrove"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      },
      {
        "questionAnswers": [],
        "id": "265952",
        "title": "Trust, Knowledge and your Data. Our approach at KADA to building a great data product",
        "description": "Have you spent time building a great data product that failed to gain traction? \r\n\r\nOr found users using a legacy report despite a better report being available?\r\n\r\nIt's a common occurrence. \r\n\r\nThrough our journey at KADA, we have identified 5 factors that make a great, trusted data product. \r\n\r\nIn this talk, I will share how you can improve your data products and show you how we built these features into K, our platform for making trust & knowledge a key part of the modern data stack. ",
        "startsAt": null,
        "endsAt": null,
        "isServiceSession": false,
        "isPlenumSession": false,
        "speakers": [
          {
            "id": "8e002909-1ba1-4662-b233-7b0aa4e791d0",
            "name": "Dean Nguyen"
          }
        ],
        "categories": [],
        "roomId": null,
        "room": null,
        "liveUrl": null,
        "recordingUrl": null
      }
    ]
  }
]